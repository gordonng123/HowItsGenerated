{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4120 Final Project -- Generating *How It's Made* Scripts\n",
    "*By Nathaniel Gordon*\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Generating the Dataset](#generating_the_dataset)\n",
    "- [Character-Level Text Generation With a RNN](#rnn)\n",
    "- [Text Generation with gpt-2-simple](#gpt2)\n",
    "\n",
    "**Note:** The GPT-2 model was originally run in a google colab instance. To get it to run in this notebook, make sure you are using Python 3.6 as it is the latest version to support the dependencies of gpt-2-simple (namely, Tensorflow 1.15.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='generating_the_dataset'></a>\n",
    "## Generating the Dataset\n",
    "\n",
    "The dataset I crafted for this project has been included as a folder of individual script files. Each script contains the text of one segment of a How It's Made episode, with the filename being the topic of focus. This script concatenates the files -- each topic is delineated with a `<BOF>` and `<EOF>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the dataset\n",
    "dataset_path = \"../data/HIM_scripts_dataset.zip\"\n",
    "\n",
    "# Open the zip file in READ mode\n",
    "with ZipFile(dataset_path, 'r') as zip:\n",
    "    script_files = zip.namelist()\n",
    "    random.shuffle(script_files)\n",
    "    data = ''\n",
    "\n",
    "    # Iterate through each file in the archive\n",
    "    for path in script_files:\n",
    "        text = zip.read(path).decode(\"utf-8\").strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        \n",
    "        # Append \"beggining of script\" and \"end of script\" key tokens\n",
    "        bos_token = '<BOS>'\n",
    "        eos_token = '<EOS>'\n",
    "        \n",
    "        data += bos_token + ' ' + text + ' ' + eos_token + '\\n'\n",
    "            \n",
    "    # Write to destination file\n",
    "    with open('../data/compiled_scripts.txt', 'w') as file: \n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnn'></a>\n",
    "## Character-Level Text Generation With a RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "import keras.models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the compiled text\n",
    "with open('data/compiled_scripts.txt', 'r') as file:\n",
    "            input_text = file.read()\n",
    "        \n",
    "print(input_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all tokens present in the sample text (we will examine novel tokens later)\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_tokens = list(set(word_tokenize(input_text)))\n",
    "\n",
    "print(all_tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform character-level text generation, an encoding is derived from the charset present in the text. Then, input sequences are derived from the training data at a given length and offset (I found a 40-3 scheme worked fairly well). The inputs are the encoded characters, with the label being the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up character encodings\n",
    "chars = sorted(list(set(input_text)))\n",
    "print('total chars: ', len(input_text))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences\n",
    "\n",
    "# Parameters\n",
    "maxlen = 40   # length of inputs\n",
    "step = 3      # offset of input vectors (overlap of maxlen - step)\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(input_text) - maxlen, step):\n",
    "    sentences.append(input_text[i: i + maxlen])\n",
    "    next_chars.append(input_text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode inputs\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture I settled on uses 3 LSTM layers, each paired with a dropout layer to reduce overfitting. The output layer uses a softmax activation unit, and the ADAM optimization scheme was used for reducing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RNN with 3 layers of LSTM nodes\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation preview helper\n",
    "#\n",
    "# Arguments:\n",
    "#   preds -- an array of probabilities\n",
    "#   temperature -- sample predictability\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function invoked at end of each epoch. Prints generated sample text at a variety of model temperatures.\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(input_text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = input_text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        \n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "        print('----- Novel words')\n",
    "        novel_words = []\n",
    "        generated_tokens = list(set(word_tokenize(generated)))\n",
    "        \n",
    "        for tok in generated_tokens:\n",
    "            if tok not in all_tokens:\n",
    "                novel_words.append(tok)\n",
    "                \n",
    "        print(novel_words)\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function for saving model checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "saved_weights_path = \"models/RNN/weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(saved_weights_path, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function for loss calculations\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "callbacks = [print_callback, checkpoint, reduce_lr]\n",
    "\n",
    "# If running from a saved checkpoint:\n",
    "model = keras.models.load_model(saved_weights_path)\n",
    "\n",
    "model.fit(x, y, batch_size=64, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to generate text from a random seed\n",
    "#\n",
    "# Arguments:\n",
    "#   length -- the number of characters to generate\n",
    "#   diversity -- how likely the model will make a sub-optimal decision\n",
    "def generate_text(length, diversity):\n",
    "    generated = ''\n",
    "    start_index = random.randint(0, len(input_text) - maxlen - 1)\n",
    "    sentence = input_text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample sentences\n",
    "for i in range(10):\n",
    "    \n",
    "    print('----- Sample text')\n",
    "    sample_text = generate_text(750, 0.7)\n",
    "    print(sample_text)\n",
    "    \n",
    "    print('----- Novel words')\n",
    "    novel_words = []\n",
    "    generated_tokens = list(set(word_tokenize(sample_text)))\n",
    "    \n",
    "    for tok in generated_tokens:\n",
    "        if tok not in all_tokens:\n",
    "            novel_words.append(tok)\n",
    "\n",
    "    print(novel_words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpt2'></a>\n",
    "## Text Generation with gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gpt-2-simple library includes several pre-trained models and scripts for fine-tuning. Below are the configurations I used to fine-tune the model and generate text.\n",
    "\n",
    "**Note:** During my experiments, I ran this section on Google Colab instance with a T4 GPU, which ran 1000 steps in approximately 50 minutes. Performance on my local hardware was significantly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Imports\n",
    "!pip install tensorflow==1.15.2\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 355M-parameter model\n",
    "gpt2.download_gpt2(model_name=\"355M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin a training session with the custom dataset\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              dataset='data/compiled_scripts.txt',\n",
    "              model_name='355M',\n",
    "              steps=1000,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=20,\n",
    "              save_every=500,\n",
    "              learning_rate=1e-5\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a training session from a checkpoint\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from a saved model checkpoint\n",
    "gpt2.generate(sess,\n",
    "              run_name='run1',\n",
    "              length=400,\n",
    "              temperature=0.3,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              prefix=\"<BOF> SEED TEXT HERE\",\n",
    "              truncate=\"<EOF>\",\n",
    "              include_prefix=False\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
